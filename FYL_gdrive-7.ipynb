{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install faster_whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DALXPlYHDqI",
        "outputId": "6aaa9844-bcd5-4848-df1e-92043f25e12e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faster_whisper\n",
            "  Downloading faster_whisper-1.2.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting ctranslate2<5,>=4.0 (from faster_whisper)\n",
            "  Downloading ctranslate2-4.6.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.21 in /usr/local/lib/python3.12/dist-packages (from faster_whisper) (0.36.0)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster_whisper) (0.22.1)\n",
            "Collecting onnxruntime<2,>=1.14 (from faster_whisper)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting av>=11 (from faster_whisper)\n",
            "  Downloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from faster_whisper) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster_whisper) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster_whisper) (2.0.2)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster_whisper) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster_whisper) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster_whisper) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster_whisper) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster_whisper) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster_whisper) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster_whisper) (1.2.0)\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster_whisper)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster_whisper) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster_whisper) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster_whisper) (1.13.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster_whisper)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster_whisper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster_whisper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster_whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster_whisper) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime<2,>=1.14->faster_whisper) (1.3.0)\n",
            "Downloading faster_whisper-1.2.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (40.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.6.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (38.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m130.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, ctranslate2, av, coloredlogs, onnxruntime, faster_whisper\n",
            "Successfully installed av-16.0.1 coloredlogs-15.0.1 ctranslate2-4.6.1 faster_whisper-1.2.1 humanfriendly-10.0 onnxruntime-1.23.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAHCuuLAXaSp",
        "outputId": "0d4c5c5e-4c23-4cde-9b5b-e4759e2c6ea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/7 Mounting Drive…\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "   IN:  /content/drive/MyDrive/asr_in\n",
            "   OUT: /content/drive/MyDrive/asr_out\n",
            "   CACHE: /content/drive/MyDrive/.ctranslate2-cache\n",
            "1/7 Install packages…\n",
            "2/7 Cache Whisper model…\n",
            "   Model cache → OK for Systran/faster-distil-whisper-large-v2 (/content/drive/MyDrive/.ctranslate2-cache/models--Systran--faster-distil-whisper-large-v2/snapshots/fe9b404fc56de3f7c38606ef9ba6fd83526d05e4)\n",
            "3/7 Preparing input…\n",
            "   File: /content/drive/MyDrive/asr_in/audiostorto.mp3\n",
            "4/7 Transcription…\n",
            "   Model cache → OK for Systran/faster-distil-whisper-large-v2 (/content/drive/MyDrive/.ctranslate2-cache/models--Systran--faster-distil-whisper-large-v2/snapshots/fe9b404fc56de3f7c38606ef9ba6fd83526d05e4)\n",
            "   OK. language=it duration≈28.5 min device=cuda compute=int8_float16\n",
            "5/7 Gemini Lesson Notes…\n",
            "   Notes → /content/drive/MyDrive/asr_out/audiostorto/audiostorto_lezione.md\n",
            "6/7 Auto compression & cleanup in asr_in…\n",
            "   Compressing MP3 → Opus (64 kbps)\n",
            "   Deleted original → audiostorto.mp3\n",
            "   Metadata → /content/drive/MyDrive/asr_out/audiostorto/meta.json\n",
            "7/7 Done. Total time: 87.6 s\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "import os, sys, shutil, subprocess, time, json, re\n",
        "from typing import Optional, Iterable\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from faster_whisper import WhisperModel\n",
        "from faster_whisper.utils import download_model\n",
        "# ================== CONFIG ==================\n",
        "FW_MODEL      = os.environ.get(\"FW_MODEL\", \"Systran/faster-distil-whisper-large-v2\")\n",
        "FORCE_LANG    = None          # \"it\",\"en\", or None for autodetect\n",
        "CHUNK_SECONDS = 30\n",
        "USE_VAD       = True\n",
        "BEAM_SIZE     = 3\n",
        "WORD_TS       = False\n",
        "GEMINI_MODEL  = os.environ.get(\"GEMINI_MODEL\", \"gemini-flash-latest\")\n",
        "MAX_MD_CHARS  = 130_000\n",
        "ENABLE_SEARCH_DEFAULT = False  # set True only if you need web tools\n",
        "ALLOWED_EXTS  = {\".wav\", \".mp3\", \".flac\"}\n",
        "DRIVE_MOUNT   = \"/content/drive\"\n",
        "DRIVE_IN_DIR  = f\"{DRIVE_MOUNT}/MyDrive/asr_in\"\n",
        "DRIVE_OUT_DIR = f\"{DRIVE_MOUNT}/MyDrive/asr_out\"\n",
        "MODEL_CACHE   = f\"{DRIVE_MOUNT}/MyDrive/.ctranslate2-cache\"\n",
        "LOCAL_IN_DIR  = \"/content/_in\"\n",
        "\n",
        "# ================== CT2 (cache) helpers ==================\n",
        "def _is_ct2_dir(path: str) -> bool:\n",
        "    \"\"\"Valid CT2 layouts:\n",
        "    - distilled: model.bin (+ config/tokenizer)\n",
        "    - split:     encoder*.bin + decoder*.bin (+ config)\n",
        "    \"\"\"\n",
        "    if not (os.path.isdir(path) and any(os.scandir(path))):\n",
        "        return False\n",
        "    files = {f.name for f in os.scandir(path) if f.is_file()}\n",
        "    has_model_bin = (\"model.bin\" in files) or any(n.startswith(\"model.\") and n.endswith(\".bin\") for n in files)\n",
        "    has_split = any(n.startswith(\"encoder\") and n.endswith(\".bin\") for n in files) and \\\n",
        "                any(n.startswith(\"decoder\") and n.endswith(\".bin\") for n in files)\n",
        "    has_cfg = (\"config.json\" in files) or (\"tokenizer.json\" in files)\n",
        "    return (has_model_bin or has_split) and has_cfg\n",
        "\n",
        "def _find_ct2_dir(base: str) -> Optional[str]:\n",
        "    if not os.path.isdir(base):\n",
        "        return None\n",
        "    if _is_ct2_dir(base):\n",
        "        return base\n",
        "    for root, dirs, files in os.walk(base):\n",
        "        if _is_ct2_dir(root):\n",
        "            return root\n",
        "    return None\n",
        "\n",
        "def _print_tree(path: str, max_items: int = 60) -> None:\n",
        "    shown = 0\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        rel = os.path.relpath(root, path)\n",
        "        print(f\"   [{rel}]\")\n",
        "        for d in dirs:\n",
        "            print(f\"     <DIR> {d}\")\n",
        "            shown += 1\n",
        "            if shown >= max_items: return\n",
        "        for f in files:\n",
        "            print(f\"           {f}\")\n",
        "            shown += 1\n",
        "            if shown >= max_items: return\n",
        "\n",
        "def _is_repo_id(model_id: str) -> bool:\n",
        "    return \"/\" in model_id  # e.g. \"Systran/faster-distil-whisper-large-v2\"\n",
        "\n",
        "def _hf_repo_cache_root(cache_dir: str, repo_id: str) -> str:\n",
        "    # HF cache convention: models--ORG--REPO\n",
        "    return os.path.join(cache_dir, \"models--\" + repo_id.replace(\"/\", \"--\"))\n",
        "\n",
        "def _find_ct2_dir_for_repo(cache_dir: str, repo_id: str) -> Optional[str]:\n",
        "    repo_root = _hf_repo_cache_root(cache_dir, repo_id)\n",
        "    if not os.path.isdir(repo_root):\n",
        "        return None\n",
        "    # usually .../snapshots/<rev>/...\n",
        "    return _find_ct2_dir(repo_root)\n",
        "\n",
        "# ================== Drive ==================\n",
        "def mount_drive() -> None:\n",
        "    from google.colab import drive\n",
        "    drive.mount(DRIVE_MOUNT)\n",
        "    for p in (DRIVE_IN_DIR, DRIVE_OUT_DIR, MODEL_CACHE, LOCAL_IN_DIR):\n",
        "        os.makedirs(p, exist_ok=True)\n",
        "\n",
        "# ================== Deps ==================\n",
        "def pip_install() -> None:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\",\n",
        "                           \"faster-whisper>=1.0.0\",\n",
        "                           \"google-genai>=0.2.0\"])\n",
        "    # avoid import confusion with the old SDK\n",
        "    subprocess.call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"google-generativeai\"],\n",
        "                    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "def ensure_model_cached() -> str:\n",
        "    \"\"\"Download FW_MODEL if missing and return the *repo-specific* CT2 folder.\"\"\"\n",
        "    os.makedirs(MODEL_CACHE, exist_ok=True)\n",
        "\n",
        "    if _is_repo_id(FW_MODEL):\n",
        "        existing = _find_ct2_dir_for_repo(MODEL_CACHE, FW_MODEL)\n",
        "        if existing:\n",
        "            print(f\"   Model cache → OK for {FW_MODEL} ({existing})\")\n",
        "            return existing\n",
        "\n",
        "    print(f\"   Model cache missing for {FW_MODEL} → downloading once\")\n",
        "    prev = os.environ.get(\"HF_HUB_OFFLINE\", \"1\")\n",
        "    os.environ[\"HF_HUB_OFFLINE\"] = \"0\"\n",
        "    try:\n",
        "        local_dir = download_model(FW_MODEL, cache_dir=MODEL_CACHE, local_files_only=False)\n",
        "    finally:\n",
        "        os.environ[\"HF_HUB_OFFLINE\"] = prev\n",
        "\n",
        "    ct2_dir = _find_ct2_dir_for_repo(MODEL_CACHE, FW_MODEL) or _find_ct2_dir(local_dir)\n",
        "    if not ct2_dir:\n",
        "        print(\"   DEBUG tree:\")\n",
        "        _print_tree(local_dir)\n",
        "        raise RuntimeError(f\"Incomplete CT2 under {local_dir} for FW_MODEL='{FW_MODEL}'\")\n",
        "\n",
        "    print(f\"   Model cache ready → {ct2_dir}\")\n",
        "    return ct2_dir\n",
        "\n",
        "# ================== Audio I/O ==================\n",
        "def pick_latest_from_drive() -> Optional[str]:\n",
        "    if not os.path.isdir(DRIVE_IN_DIR):\n",
        "        return None\n",
        "    cands = []\n",
        "    for name in os.listdir(DRIVE_IN_DIR):\n",
        "        p = os.path.join(DRIVE_IN_DIR, name)\n",
        "        if os.path.isfile(p) and os.path.splitext(name)[1].lower() in ALLOWED_EXTS:\n",
        "            cands.append(p)\n",
        "    if not cands:\n",
        "        return None\n",
        "    cands.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
        "    return cands[0]\n",
        "\n",
        "def prepare_audio() -> tuple[str, str, str]:\n",
        "    src = pick_latest_from_drive()\n",
        "    if not src:\n",
        "        present = []\n",
        "        if os.path.isdir(DRIVE_IN_DIR):\n",
        "            present = [n for n in os.listdir(DRIVE_IN_DIR) if os.path.isfile(os.path.join(DRIVE_IN_DIR, n))]\n",
        "        raise FileNotFoundError(\n",
        "            f\"Nessun file valido in {DRIVE_IN_DIR}. Metti .wav/.mp3/.flac e riesegui. \"\n",
        "            f\"Presenti: {present or '—'}\"\n",
        "        )\n",
        "    base = os.path.basename(src)\n",
        "    name, ext = os.path.splitext(base)\n",
        "    ext = ext.lower()\n",
        "    if ext not in ALLOWED_EXTS:\n",
        "        raise ValueError(f\"Estensione non supportata: {ext}\")\n",
        "    local_path = os.path.join(LOCAL_IN_DIR, f\"{name}{ext}\")\n",
        "    if os.path.exists(local_path):\n",
        "        os.remove(local_path)\n",
        "    shutil.copyfile(src, local_path)\n",
        "    return src, name, local_path\n",
        "\n",
        "def run_dir_for(base_name: str) -> str:\n",
        "    rd = os.path.join(DRIVE_OUT_DIR, base_name)\n",
        "    os.makedirs(rd, exist_ok=True)\n",
        "    return rd\n",
        "\n",
        "def detect_device() -> str:\n",
        "    try:\n",
        "        import torch\n",
        "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    except Exception:\n",
        "        return \"cpu\"\n",
        "\n",
        "# ================== Transcribe ==================\n",
        "def transcribe(input_audio_path: str, out_txt_path: str) -> dict:\n",
        "    device = detect_device()\n",
        "    compute_type = \"int8_float16\" if device == \"cuda\" else \"int8\"\n",
        "\n",
        "    local_model_dir = ensure_model_cached()\n",
        "\n",
        "    model = WhisperModel(\n",
        "        local_model_dir,              # use resolved CT2 path (not a model name)\n",
        "        device=device,\n",
        "        compute_type=compute_type,\n",
        "        download_root=MODEL_CACHE,\n",
        "        cpu_threads=os.cpu_count()\n",
        "    )\n",
        "\n",
        "    segments_iter, info = model.transcribe(\n",
        "        input_audio_path,\n",
        "        language=FORCE_LANG,\n",
        "        vad_filter=USE_VAD,\n",
        "        vad_parameters={\"min_silence_duration_ms\": 300},\n",
        "        beam_size=BEAM_SIZE,\n",
        "        chunk_length=CHUNK_SECONDS,\n",
        "        condition_on_previous_text=False,\n",
        "        word_timestamps=WORD_TS,\n",
        "        no_speech_threshold=0.6,\n",
        "        compression_ratio_threshold=2.6,\n",
        "        log_prob_threshold=-1.0\n",
        "    )\n",
        "\n",
        "    with open(out_txt_path, \"w\", encoding=\"utf-8\") as tf:\n",
        "        buf = []\n",
        "        for seg in segments_iter:\n",
        "            t = (seg.text or \"\").strip()\n",
        "            if t:\n",
        "                buf.append(t)\n",
        "        text = \" \".join(buf)\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "        text = re.sub(r\"\\s+([.!?…])\", r\"\\1\", text)\n",
        "        tf.write(text + \"\\n\")\n",
        "\n",
        "    if not text:\n",
        "        raise RuntimeError(\"Empty transcript: no speech detected.\")\n",
        "\n",
        "    return {\n",
        "        \"language\": getattr(info, \"language\", FORCE_LANG),\n",
        "        \"duration_min\": round(getattr(info, \"duration\", 0)/60, 1),\n",
        "        \"device\": device,\n",
        "        \"compute\": compute_type,\n",
        "    }\n",
        "\n",
        "# ================== Gemini ==================\n",
        "def _get_gemini_api_key_any() -> str:\n",
        "    key = os.environ.get(\"GEMINI_API_KEY\", \"\")\n",
        "    if key:\n",
        "        return key\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "        if key:\n",
        "            os.environ[\"GEMINI_API_KEY\"] = key\n",
        "            return key\n",
        "    except Exception:\n",
        "        pass\n",
        "    raise RuntimeError(\"API key mancante. Imposta GEMINI_API_KEY o Colab Secret GOOGLE_API_KEY.\")\n",
        "\n",
        "def _smart_chunks(txt: str, max_chars: int = MAX_MD_CHARS) -> list[str]:\n",
        "    if len(txt) <= max_chars:\n",
        "        return [txt]\n",
        "    parts, buf, size = [], [], 0\n",
        "    for para in re.split(r\"(\\n{2,})\", txt):\n",
        "        if size + len(para) > max_chars and buf:\n",
        "            parts.append(\"\".join(buf)); buf, size = [], 0\n",
        "        buf.append(para); size += len(para)\n",
        "    if buf:\n",
        "        parts.append(\"\".join(buf))\n",
        "    return parts\n",
        "\n",
        "def _lesson_system_instruction(lang_hint: str | None) -> str:\n",
        "    return f\"\"\"You are an accurate note-taker. Convert the transcript into clean LESSON NOTES in Markdown.\n",
        "- Keep the original language; if unclear use '{lang_hint or \"auto\"}'.\n",
        "- No timestamps. Do not invent content. Keep technical terms verbatim.\n",
        "- Prefer bullet points and short lines.\n",
        "Structure:\n",
        "# Titolo sintetico\n",
        "## Obiettivi della lezione\n",
        "## Outline (sequenza degli argomenti)\n",
        "## Concetti chiave\n",
        "## Esempi/Case\n",
        "## Termini e definizioni (formato **Termine:** definizione)\n",
        "## Citazioni utili (tra virgolette)\n",
        "## Domande aperte\n",
        "## Action items / Compiti\n",
        "Output: valid Markdown only.\n",
        "\"\"\".strip()\n",
        "\n",
        "def _stream_text(chunks: Iterable) -> str:\n",
        "    out = []\n",
        "    for ch in chunks:\n",
        "        t = getattr(ch, \"text\", None)\n",
        "        if t:\n",
        "            out.append(t)\n",
        "    return \"\".join(out)\n",
        "\n",
        "def generate_notes_with_genai(raw_text: str,\n",
        "                              lang_hint: str | None = None,\n",
        "                              enable_search: bool | None = None) -> str:\n",
        "    _get_gemini_api_key_any()\n",
        "    client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "    system_instruction = [types.Part.from_text(text=_lesson_system_instruction(lang_hint))]\n",
        "    chunks = _smart_chunks(raw_text, max_chars=MAX_MD_CHARS)\n",
        "    if enable_search is None:\n",
        "        enable_search = ENABLE_SEARCH_DEFAULT\n",
        "\n",
        "    tools = []\n",
        "    if enable_search:\n",
        "        tools = [\n",
        "            types.Tool(url_context=types.UrlContext()),\n",
        "            types.Tool(googleSearch=types.GoogleSearch()),\n",
        "        ]\n",
        "\n",
        "    md_parts = []\n",
        "    for i, ch in enumerate(chunks, 1):\n",
        "        contents = [types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[types.Part.from_text(\n",
        "                text=f\"Part {i}/{len(chunks)}. Apply the system instruction and produce LESSON NOTES.\\n\\n{ch}\"\n",
        "            )],\n",
        "        )]\n",
        "        base_config = types.GenerateContentConfig(\n",
        "            system_instruction=system_instruction,\n",
        "            temperature=0.2,\n",
        "            top_p=0.95,\n",
        "            max_output_tokens=8192,\n",
        "            thinking_config=types.ThinkingConfig(thinking_budget=1200),\n",
        "        )\n",
        "        try:\n",
        "            cfg = base_config if not tools else types.GenerateContentConfig(\n",
        "                system_instruction=system_instruction,\n",
        "                temperature=base_config.temperature,\n",
        "                top_p=base_config.top_p,\n",
        "                max_output_tokens=base_config.max_output_tokens,\n",
        "                thinking_config=base_config.thinking_config,\n",
        "                tools=tools,\n",
        "            )\n",
        "            stream = client.models.generate_content_stream(\n",
        "                model=GEMINI_MODEL, contents=contents, config=cfg\n",
        "            )\n",
        "            md = _stream_text(stream).strip()\n",
        "        except Exception:\n",
        "            # fallback without tools\n",
        "            stream = client.models.generate_content_stream(\n",
        "                model=GEMINI_MODEL, contents=contents, config=base_config\n",
        "            )\n",
        "            md = _stream_text(stream).strip()\n",
        "\n",
        "        if not md:\n",
        "            raise RuntimeError(\"Gemini returned empty text.\")\n",
        "        md_parts.append(md)\n",
        "\n",
        "    merged = (\"\\n\\n\".join(md_parts)).strip()\n",
        "    return (merged + \"\\n\") if not merged.endswith(\"\\n\") else merged\n",
        "\n",
        "# ================== State / Resume (per-run subfolder) ==================\n",
        "def run_dir_for(base_name: str) -> str:\n",
        "    rd = os.path.join(DRIVE_OUT_DIR, base_name)\n",
        "    os.makedirs(rd, exist_ok=True)\n",
        "    return rd\n",
        "\n",
        "def _state_path(base_name: str) -> str:\n",
        "    return os.path.join(run_dir_for(base_name), \"state.json\")\n",
        "\n",
        "def save_state(base_name: str, status: str, meta: dict, out_txt: str, out_md: str, src_audio_drive_path: str | None = None):\n",
        "    state = {\n",
        "        \"base_name\": base_name,\n",
        "        \"status\": status,  # TRANSCRIBED_READY_FOR_NOTES | DONE\n",
        "        \"out_txt\": out_txt,\n",
        "        \"out_md\": out_md,\n",
        "        \"meta\": meta,\n",
        "        \"src_audio_drive_path\": src_audio_drive_path,\n",
        "        \"ts\": time.time(),\n",
        "    }\n",
        "    with open(_state_path(base_name), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(state, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def load_pending_state() -> Optional[dict]:\n",
        "    if not os.path.isdir(DRIVE_OUT_DIR):\n",
        "        return None\n",
        "    pend = []\n",
        "    for name in os.listdir(DRIVE_OUT_DIR):\n",
        "        st_path = os.path.join(DRIVE_OUT_DIR, name, \"state.json\")\n",
        "        if os.path.isfile(st_path):\n",
        "            try:\n",
        "                with open(st_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    st = json.load(f)\n",
        "                if st.get(\"status\") == \"TRANSCRIBED_READY_FOR_NOTES\":\n",
        "                    pend.append((os.path.getmtime(st_path), st))\n",
        "            except Exception:\n",
        "                pass\n",
        "    if not pend:\n",
        "        return None\n",
        "    pend.sort(key=lambda x: x[0], reverse=True)\n",
        "    return pend[0][1]\n",
        "\n",
        "# ================== Auto-compression (asr_in) ==================\n",
        "def auto_compress_audio(src_drive_path: str) -> Optional[str]:\n",
        "    if not (src_drive_path and os.path.isfile(src_drive_path)):\n",
        "        return None\n",
        "\n",
        "    in_dir = os.path.dirname(src_drive_path)\n",
        "    base = os.path.basename(src_drive_path)\n",
        "    name, ext = os.path.splitext(base)\n",
        "    ext = ext.lower()\n",
        "\n",
        "    if ext == \".wav\":\n",
        "        dst_path = os.path.join(in_dir, f\"{name}.flac\")\n",
        "        print(\"   Compressing WAV → FLAC (lossless)\")\n",
        "        cmd = [\"ffmpeg\", \"-y\", \"-i\", src_drive_path, \"-compression_level\", \"5\", dst_path]\n",
        "    elif ext == \".mp3\":\n",
        "        dst_path = os.path.join(in_dir, f\"{name}.opus\")\n",
        "        print(\"   Compressing MP3 → Opus (64 kbps)\")\n",
        "        cmd = [\"ffmpeg\", \"-y\", \"-i\", src_drive_path, \"-b:a\", \"64k\", dst_path]\n",
        "    elif ext == \".flac\":\n",
        "        print(\"   Source already FLAC — no compression needed.\")\n",
        "        return src_drive_path\n",
        "    else:\n",
        "        print(f\"   Unsupported extension for compression: {ext}\")\n",
        "        return src_drive_path\n",
        "\n",
        "    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
        "\n",
        "    try:\n",
        "        os.remove(src_drive_path)\n",
        "        print(f\"   Deleted original → {base}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   Warning: could not delete original ({e.__class__.__name__})\")\n",
        "\n",
        "    return dst_path\n",
        "\n",
        "# ================== MAIN ==================\n",
        "t0 = time.perf_counter()\n",
        "print(\"0/7 Mounting Drive…\"); mount_drive()\n",
        "print(f\"   IN:  {DRIVE_IN_DIR}\\n   OUT: {DRIVE_OUT_DIR}\\n   CACHE: {MODEL_CACHE}\")\n",
        "\n",
        "print(\"1/7 Install packages…\"); pip_install()\n",
        "print(\"2/7 Cache Whisper model…\"); ensure_model_cached()\n",
        "\n",
        "resume = load_pending_state()\n",
        "if resume:\n",
        "    print(\"3/7 Resume found → skip transcription.\")\n",
        "    base_name = resume[\"base_name\"]\n",
        "    run_dir   = run_dir_for(base_name)\n",
        "    out_txt   = resume[\"out_txt\"]\n",
        "    out_md    = resume[\"out_md\"]\n",
        "    local_audio = None\n",
        "    meta = resume.get(\"meta\", {})\n",
        "    src_path = resume.get(\"src_audio_drive_path\")\n",
        "else:\n",
        "    print(\"3/7 Preparing input…\")\n",
        "    src_path, base_name, local_audio = prepare_audio()\n",
        "    run_dir = run_dir_for(base_name)\n",
        "    out_txt = os.path.join(run_dir, f\"{base_name}.txt\")\n",
        "    out_md  = os.path.join(run_dir, f\"{base_name}_lezione.md\")\n",
        "    print(f\"   File: {src_path}\")\n",
        "\n",
        "    print(\"4/7 Transcription…\")\n",
        "    meta = transcribe(local_audio, out_txt)\n",
        "    print(f\"   OK. language={meta['language']} duration≈{meta['duration_min']} min device={meta['device']} compute={meta['compute']}\")\n",
        "    save_state(base_name, \"TRANSCRIBED_READY_FOR_NOTES\", meta, out_txt, out_md, src_audio_drive_path=src_path)\n",
        "\n",
        "if resume:\n",
        "    print(\"4/7 Transcription… [SKIP]\")\n",
        "\n",
        "print(\"5/7 Gemini Lesson Notes…\")\n",
        "try:\n",
        "    with open(out_txt, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw = f.read().strip()\n",
        "    notes_md = generate_notes_with_genai(raw_text=raw, lang_hint=(FORCE_LANG or \"auto\"), enable_search=False)\n",
        "    with open(out_md, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(notes_md)\n",
        "    save_state(base_name, \"DONE\", meta, out_txt, out_md, src_audio_drive_path=src_path)\n",
        "    print(f\"   Notes → {out_md}\")\n",
        "except Exception as e:\n",
        "    print(\"   ERROR in Notes step — state saved. You can retry from the .txt\")\n",
        "    print(\"   Details:\", repr(e))\n",
        "    print(\"7/7 Partial. Total time:\", round(time.perf_counter()-t0, 1), \"s\")\n",
        "    sys.exit(0)\n",
        "\n",
        "print(\"6/7 Auto compression & cleanup in asr_in…\")\n",
        "try:\n",
        "    if src_path:\n",
        "        new_audio_path = auto_compress_audio(src_path)\n",
        "        if new_audio_path:\n",
        "            meta[\"compressed_audio\"] = new_audio_path\n",
        "except Exception as e:\n",
        "    print(f\"   Compression skipped ({e.__class__.__name__}): {e}\")\n",
        "\n",
        "# Persist meta.json to the run subfolder\n",
        "try:\n",
        "    with open(os.path.join(run_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"   Metadata → {os.path.join(run_dir, 'meta.json')}\")\n",
        "except Exception as e:\n",
        "    print(f\"   Warning: could not write meta.json ({e.__class__.__name__})\")\n",
        "\n",
        "print(\"7/7 Done. Total time:\", round(time.perf_counter()-t0, 1), \"s\")"
      ]
    }
  ]
}